{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3342171,"sourceType":"datasetVersion","datasetId":2017696},{"sourceId":164663,"sourceType":"modelInstanceVersion","modelInstanceId":140083,"modelId":162707},{"sourceId":164664,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":140084,"modelId":162708}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Originally tried to scale 32x32->128x128, but it seems like that is too difficult of a task to do on free GPUs. The GAN optimizes to a local minima and fails to find the HD image after multiple epochs. Instead we attempt 64x64->128x128","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-24T23:28:01.287913Z","iopub.execute_input":"2024-12-24T23:28:01.288328Z","iopub.status.idle":"2024-12-24T23:28:01.631030Z","shell.execute_reply.started":"2024-12-24T23:28:01.288292Z","shell.execute_reply":"2024-12-24T23:28:01.630358Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import InterpolationMode\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-12-24T23:28:18.803642Z","iopub.execute_input":"2024-12-24T23:28:18.804532Z","iopub.status.idle":"2024-12-24T23:28:23.166860Z","shell.execute_reply.started":"2024-12-24T23:28:18.804482Z","shell.execute_reply":"2024-12-24T23:28:23.166016Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-12-24T23:28:28.523070Z","iopub.execute_input":"2024-12-24T23:28:28.523589Z","iopub.status.idle":"2024-12-24T23:28:28.574615Z","shell.execute_reply.started":"2024-12-24T23:28:28.523558Z","shell.execute_reply":"2024-12-24T23:28:28.573673Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"torch.manual_seed(42)\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-12-24T23:28:31.279358Z","iopub.execute_input":"2024-12-24T23:28:31.279701Z","iopub.status.idle":"2024-12-24T23:28:31.290491Z","shell.execute_reply.started":"2024-12-24T23:28:31.279668Z","shell.execute_reply":"2024-12-24T23:28:31.289584Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## data pipeline","metadata":{}},{"cell_type":"code","source":"class TrainValDataset(Dataset):\n    def __init__(self, hr_dir, file_list, scale_factor=2, crop_size=128):\n        self.hr_dir = hr_dir\n        self.file_list = file_list\n        self.scale_factor = scale_factor\n        self.crop_size = crop_size\n        \n        # validate files exist\n        self.valid_files = []\n        for file in file_list:\n            full_path = os.path.join(hr_dir, file)\n            if os.path.exists(full_path):\n                self.valid_files.append(file)\n        \n        if len(self.valid_files) == 0:\n            raise ValueError(f\"No valid image files found in {hr_dir}\")\n        \n        print(f\"Found {len(self.valid_files)} valid images\")\n    \n    def __len__(self):\n        return len(self.valid_files)\n    \n    def __getitem__(self, idx):\n        # load high-resolution image\n        img_path = os.path.join(self.hr_dir, self.valid_files[idx])\n        hr_img = Image.open(img_path).convert('RGB')\n        \n        # apply random crop to PIL image first\n        transform_crop = transforms.RandomCrop(self.crop_size)\n        hr_img_cropped = transform_crop(hr_img)\n        \n        # convert cropped HR image to tensor\n        hr_tensor = transforms.ToTensor()(hr_img_cropped)\n        \n        # create low-resolution version from the cropped image\n        lr_size = self.crop_size // self.scale_factor\n        lr_tensor = transforms.Resize((lr_size, lr_size), \n                                    interpolation=InterpolationMode.BICUBIC)(hr_tensor.unsqueeze(0)).squeeze(0)\n        \n        return lr_tensor, hr_tensor\n\ndef create_dataloaders(hr_dir, batch_size=16, scale_factor=2, crop_size=128, num_workers=2):\n    \"\"\"Create training and validation dataloaders\"\"\"\n    # check if directory exists\n    if not os.path.exists(hr_dir):\n        raise ValueError(f\"Directory not found: {hr_dir}\")\n    \n    # get list of image files\n    image_files = []\n    for file in os.listdir(hr_dir):\n        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n            image_files.append(file)\n    \n    if not image_files:\n        raise ValueError(f\"No image files found in {hr_dir}\")\n    \n    print(f\"Total images found: {len(image_files)}\")\n    \n    # split files into train and validation\n    train_size = int(0.8 * len(image_files))\n    train_files = image_files[:train_size]\n    val_files = image_files[train_size:]\n    \n    print(f\"Training images: {len(train_files)}\")\n    print(f\"Validation images: {len(val_files)}\")\n    \n    # create datasets\n    train_dataset = TrainValDataset(hr_dir, train_files, scale_factor, crop_size)\n    val_dataset = TrainValDataset(hr_dir, val_files, scale_factor, crop_size)\n    \n    # create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True\n    )\n    \n    return train_loader, val_loader\n\ndef plot_samples(loader, num_samples=3):\n    \"\"\"Plot sample low-res and high-res image pairs\"\"\"\n    plt.figure(figsize=(15, 5))\n    \n    # get a batch of samples\n    samples = next(iter(loader))\n    lr_batch, hr_batch = samples\n    \n    for i in range(min(num_samples, lr_batch.size(0))):\n        # convert tensors to numpy arrays for plotting\n        lr_img = lr_batch[i].permute(1, 2, 0).cpu().numpy()\n        hr_img = hr_batch[i].permute(1, 2, 0).cpu().numpy()\n        \n        # clip values to valid range and adjust for display\n        lr_img = np.clip(lr_img, 0, 1)\n        hr_img = np.clip(hr_img, 0, 1)\n        \n        plt.subplot(2, num_samples, i + 1)\n        plt.imshow(lr_img)\n        plt.title(f'Low Res {lr_img.shape[:2]}')\n        plt.axis('off')\n        \n        plt.subplot(2, num_samples, i + num_samples + 1)\n        plt.imshow(hr_img)\n        plt.title(f'High Res {hr_img.shape[:2]}')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-12-24T23:28:35.559966Z","iopub.execute_input":"2024-12-24T23:28:35.560826Z","iopub.status.idle":"2024-12-24T23:28:35.576189Z","shell.execute_reply.started":"2024-12-24T23:28:35.560784Z","shell.execute_reply":"2024-12-24T23:28:35.575239Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Used CNN architecture","metadata":{}},{"cell_type":"code","source":"class UpsamplingCNN(nn.Module):\n    def __init__(self):\n        super(UpsamplingCNN, self).__init__()\n        \n        # encoder part: downsample the input\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),  # Down to 32x32\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2)   # Down to 16x16\n        )\n        \n        # decoder part: upsample the image to 128x128\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # Up to 32x32\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # Up to 64x64\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),     # Up to 128x128\n            nn.Sigmoid()  # output range [0, 1]\n        )\n\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n# instantiate the model and print it\nmodel = UpsamplingCNN()\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-12-24T23:28:44.853152Z","iopub.execute_input":"2024-12-24T23:28:44.853515Z","iopub.status.idle":"2024-12-24T23:28:44.891759Z","shell.execute_reply.started":"2024-12-24T23:28:44.853484Z","shell.execute_reply":"2024-12-24T23:28:44.890878Z"},"trusted":true},"outputs":[{"name":"stdout","text":"UpsamplingCNN(\n  (encoder): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (decoder): Sequential(\n    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n    (2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (3): ReLU()\n    (4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (5): Sigmoid()\n  )\n)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"## training process\n\ntrain_loader, val_loader = create_dataloaders(\n    '/kaggle/input/div2k-high-resolution-images/DIV2K_train_HR/DIV2K_train_HR',\n    batch_size=16,\n    scale_factor=2,\n    crop_size=128\n)\n\n## define the model, loss function, and optimizer\nmodel = UpsamplingCNN().to(device)  # Move model to GPU if available\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-12-24T23:28:50.792124Z","iopub.execute_input":"2024-12-24T23:28:50.792914Z","iopub.status.idle":"2024-12-24T23:28:53.281610Z","shell.execute_reply.started":"2024-12-24T23:28:50.792880Z","shell.execute_reply":"2024-12-24T23:28:53.280862Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Total images found: 800\nTraining images: 640\nValidation images: 160\nFound 640 valid images\nFound 160 valid images\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"model = UpsamplingCNN()\ndummy_input = torch.randn(16, 3, 64, 64)  # Batch of 16, 64x64 images\noutput = model(dummy_input)\nprint(output.shape)  # Should print torch.Size([16, 3, 128, 128])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T23:28:53.535613Z","iopub.execute_input":"2024-12-24T23:28:53.536366Z","iopub.status.idle":"2024-12-24T23:28:53.801373Z","shell.execute_reply.started":"2024-12-24T23:28:53.536334Z","shell.execute_reply":"2024-12-24T23:28:53.800374Z"}},"outputs":[{"name":"stdout","text":"torch.Size([16, 3, 128, 128])\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"count = 0\nfor root, folders, filenames in os.walk('/kaggle/working/.virtual_documents'):\n   print(root, folders)","metadata":{"execution":{"iopub.status.busy":"2024-12-19T22:43:19.617582Z","iopub.execute_input":"2024-12-19T22:43:19.617949Z","iopub.status.idle":"2024-12-19T22:43:19.623025Z","shell.execute_reply.started":"2024-12-19T22:43:19.617918Z","shell.execute_reply":"2024-12-19T22:43:19.622170Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/.virtual_documents []\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"## function to load the checkpoint\ndef load_checkpoint(model, optimizer, filename):\n    checkpoint = torch.load(filename,map_location=torch.device('cuda'))\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    loss = checkpoint['loss']\n    return model, optimizer, epoch, loss\n\nmodel = UpsamplingCNN()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# model, optimizer, start_epoch, start_loss = load_checkpoint(model, optimizer, filename=\"/kaggle/input/checkpoint7/pytorch/default/1/checkpoint_7.pth\")\n# print(f\"Resuming training from epoch {start_epoch+1} with loss {start_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T17:09:53.876036Z","iopub.execute_input":"2024-11-12T17:09:53.876793Z","iopub.status.idle":"2024-11-12T17:09:53.929936Z","shell.execute_reply.started":"2024-11-12T17:09:53.876752Z","shell.execute_reply":"2024-11-12T17:09:53.929021Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Resuming training from epoch 60 with loss 0.009660433186218143\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/405659625.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(filename,map_location=torch.device('cuda'))\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# print(\"Model device:\", next(model.parameters()).device)\n\n# for low_res, high_res in train_loader:\n#     low_res, high_res = low_res.to(device), high_res.to(device)\n#     print(\"Low resolution image device:\", low_res.device)\n#     print(\"High resolution image device:\", high_res.device)\n\nprint(\"CUDA available:\", torch.cuda.is_available())\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-12-24T23:29:02.502378Z","iopub.execute_input":"2024-12-24T23:29:02.503013Z","iopub.status.idle":"2024-12-24T23:29:03.617109Z","shell.execute_reply.started":"2024-12-24T23:29:02.502974Z","shell.execute_reply":"2024-12-24T23:29:03.616164Z"},"trusted":true},"outputs":[{"name":"stdout","text":"CUDA available: True\nTue Dec 24 23:29:03 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   39C    P0             26W /   70W |     105MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   37C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, epoch, loss, filename):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n    }\n    torch.save(checkpoint, filename)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T20:54:20.624760Z","iopub.execute_input":"2024-11-12T20:54:20.625665Z","iopub.status.idle":"2024-11-12T20:54:20.631360Z","shell.execute_reply.started":"2024-11-12T20:54:20.625622Z","shell.execute_reply":"2024-11-12T20:54:20.630326Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"## set device\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n## move model to the gpu once at the start\nmodel = model.to(device)\n\n\n\n## training function\ndef train(model, dataloader, criterion, optimizer, device):\n    model.to(device).train()\n    running_loss = 0.0\n    for low_res, high_res in tqdm(dataloader):\n        low_res, high_res = low_res.to(device), high_res.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(low_res)\n        loss = criterion(outputs, high_res)\n        loss.backward()\n        optimizer.step()\n#         torch.cuda.synchronize()\n\n        running_loss += loss.item()\n    return running_loss / len(dataloader)\n\n## validation function\ndef validate(model, dataloader, criterion, device):\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for low_res, high_res in dataloader:\n            low_res, high_res = low_res.to(device), high_res.to(device)\n\n            outputs = model(low_res)\n            loss = criterion(outputs, high_res)\n            val_loss += loss.item()\n    return val_loss / len(dataloader)\n\n## training loop\nnum_epochs = 20\ncheckpoint = 1\nfor epoch in range(num_epochs):\n    train_loss = train(model.to(device), train_loader, criterion, optimizer, device)\n    val_loss = validate(model.to(device), val_loader, criterion, device)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n    \n    if epoch % 20 == 0 and epoch != 0:\n        save_checkpoint(model, optimizer, epoch, val_loss, filename=\"checkpoint_{}.pth\".format(checkpoint))\n        checkpoint += 1\n        print('checkpoint saved')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"  save_checkpoint(model, optimizer, epoch, val_loss, filename=\"checkpoint_12410.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:10:57.230605Z","iopub.execute_input":"2024-11-12T22:10:57.231427Z","iopub.status.idle":"2024-11-12T22:10:57.263666Z","shell.execute_reply.started":"2024-11-12T22:10:57.231382Z","shell.execute_reply":"2024-11-12T22:10:57.262911Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import os\nprint(os.getcwd()) ","metadata":{"execution":{"iopub.status.busy":"2024-11-05T02:30:35.037878Z","iopub.execute_input":"2024-11-05T02:30:35.038317Z","iopub.status.idle":"2024-11-05T02:30:35.044417Z","shell.execute_reply.started":"2024-11-05T02:30:35.038279Z","shell.execute_reply":"2024-11-05T02:30:35.043232Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Run inference on a specific image","metadata":{}},{"cell_type":"code","source":"## function to load and preprocess a single image for inference\ndef load_image(path, input_size=(64, 64), target_size=(128, 128)):\n    img = Image.open(path).convert('RGB')\n    \n    ## transformations: downscale to 32x32 (or 64x64) for input, keep original as high-resolution\n    transform_low_res = transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.ToTensor()\n    ])\n    transform_high_res = transforms.Compose([\n        transforms.Resize(target_size),\n        transforms.ToTensor()\n    ])\n    \n    low_res_img = transform_low_res(img).unsqueeze(0)  ## add batch dimension\n    high_res_img = transform_high_res(img)  ## for comparison only (not used in inference)\n    \n    return low_res_img, high_res_img\n\n## function to run inference and display results\ndef run_inference(model, image_path):\n    ## load and preprocess the image\n    low_res_img, high_res_img = load_image(image_path)\n    low_res_img = low_res_img.cuda() if torch.cuda.is_available() else low_res_img\n    \n    ## set model to evaluation mode and run inference\n    model.eval()\n    with torch.no_grad():\n        generated_img = model(low_res_img).squeeze(0).cpu()  # Remove batch dimension\n    \n    ## convert tensors to PIL images for display\n    low_res_img = transforms.ToPILImage()(low_res_img.squeeze(0))\n    generated_img = transforms.ToPILImage()(generated_img)\n    high_res_img = transforms.ToPILImage()(high_res_img)\n    \n    ## display the original, low-resolution, and generated high-resolution images\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    axes[0].imshow(high_res_img)\n    axes[0].set_title(\"Original High-Resolution\")\n    axes[1].imshow(low_res_img)\n    axes[1].set_title(\"Low-Resolution Input (64x64)\")\n    axes[2].imshow(generated_img)\n    axes[2].set_title(\"Generated High-Resolution (128x128)\")\n    \n    for ax in axes:\n        ax.axis(\"off\")\n    plt.show()\n\n\n## assume `model` is the trained UpsamplingCNN model\nimage_path = '/kaggle/input/div2k-high-resolution-images/DIV2K_train_HR/DIV2K_train_HR/0009.png'\nrun_inference(model, image_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## discriminator network","metadata":{}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        \n        self.main = nn.Sequential(\n            # Input: (3, 128, 128)\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # (64, 64, 64)\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # (128, 32, 32)\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # (256, 16, 16)\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # (512, 8, 8)\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),\n            # Output: (1, 5, 5)\n            nn.Sigmoid()  # For binary classification (real/fake)\n        )\n        \n    def forward(self, x):\n        return self.main(x)\n\n## instantiate the discriminator\ndiscriminator = Discriminator()\nprint(discriminator)","metadata":{"execution":{"iopub.status.busy":"2024-12-19T23:03:25.263761Z","iopub.execute_input":"2024-12-19T23:03:25.264484Z","iopub.status.idle":"2024-12-19T23:03:25.305268Z","shell.execute_reply.started":"2024-12-19T23:03:25.264449Z","shell.execute_reply":"2024-12-19T23:03:25.304203Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Discriminator(\n  (main): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n    (12): Sigmoid()\n  )\n)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"## loss function\ncriterion = nn.BCELoss()\n\n## optimizer for the discriminator\noptimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\n## training loop for the discriminator\ndef train_discriminator(discriminator, generator, data_loader, device, num_epochs):\n    discriminator.train()\n    generator.eval()  ## ensure generator is in evaluation mode during D training\n\n    real_label = 1.0\n    fake_label = 0.0\n    \n    for epoch in range(num_epochs):\n        for i, (low_res, high_res) in enumerate(data_loader):\n            ## move data to the correct device\n            low_res = low_res.to(device)\n            high_res = high_res.to(device)\n\n            ## train with real images\n            optimizer_D.zero_grad()\n            output_real = discriminator(high_res).view(-1)\n            labels_real = torch.full((output_real.size(0),), real_label, dtype=torch.float, device=device)\n            loss_real = criterion(output_real, labels_real)\n            loss_real.backward()\n\n            ## generate fake images using the generator\n            with torch.no_grad():  # No need to calculate gradients for generator here\n                fake_images = generator(low_res)\n\n            ## train with fake images\n            output_fake = discriminator(fake_images.detach()).view(-1)\n            labels_fake = torch.full((output_fake.size(0),), fake_label, dtype=torch.float, device=device)\n            loss_fake = criterion(output_fake, labels_fake)\n            loss_fake.backward()\n\n            ## update the discriminator\n            optimizer_D.step()\n\n            ## calculate total loss for logging\n            loss_D = loss_real + loss_fake\n            # save_checkpoint(discriminator, optimizer_D, i, loss_D.item(), filename=\"discriminator_12420.pth\".format(checkpoint))\n            return\n\n            if i % 1 == 0:  ## print progress every n batches\n                print(f'Batch {i}/{len(data_loader)}: Loss_D: {loss_D.item():.4f}')\n    \n    \n    return loss_D.item()\n\ntrain_discriminator(discriminator.to(device), model.to(device), train_loader, device, 10)","metadata":{"execution":{"iopub.status.busy":"2024-12-19T23:04:25.987255Z","iopub.execute_input":"2024-12-19T23:04:25.987627Z","iopub.status.idle":"2024-12-19T23:04:28.871686Z","shell.execute_reply.started":"2024-12-19T23:04:25.987595Z","shell.execute_reply":"2024-12-19T23:04:28.870747Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## full GAN network","metadata":{}},{"cell_type":"code","source":"## function to load the checkpoint for generator/discriminator\ndef load_checkpoint(model, optimizer, filename):\n    checkpoint = torch.load(filename,map_location=torch.device('cuda'))\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    loss = checkpoint['loss']\n    return model, optimizer, epoch, loss\n\n\ngenerator = UpsamplingCNN()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n\n# generator, optimizer, start_epoch, start_loss = load_checkpoint(generator, optimizer, filename=\"/kaggle/input/generator_100/pytorch/default/1/checkpoint_12410.pth\")\n# print(f\"Resuming generator training from epoch {start_epoch+1} with loss {start_loss}\")\n\n# discriminator, optimizer, start_epoch, start_loss = load_checkpoint(discriminator, optimizer, filename=\"/kaggle/input/discriminator_10/pytorch/default/1/discriminator_12420.pth\")\n# print(f\"Resuming discriminator training from epoch {start_epoch+1} with loss {start_loss}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_gradient_penalty(discriminator, real_samples, fake_samples, device):\n    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n    d_interpolates = discriminator(interpolates)\n    fake = torch.ones(d_interpolates.size(), device=device, requires_grad=False)\n\n    gradients = torch.autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,  # Keep graph for further backward computation\n        only_inputs=True\n    )[0]\n\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty","metadata":{"execution":{"iopub.status.busy":"2024-12-19T23:05:39.808907Z","iopub.execute_input":"2024-12-19T23:05:39.809277Z","iopub.status.idle":"2024-12-19T23:05:39.815639Z","shell.execute_reply.started":"2024-12-19T23:05:39.809246Z","shell.execute_reply":"2024-12-19T23:05:39.814638Z"},"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":"## full training process for GAN\n\n## worked for about 5-10 epochs\n## need to make discriminator network better in some way\n\n\n## instantiate the generator and discriminator models\n# generator = UpsamplingCNN().to(device)\n\ngenerator = UpsamplingCNN()  # Or your GAN generator\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\ndiscriminator = Discriminator()\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngenerator = generator.to(device)\ndiscriminator = discriminator.to(device)\n\n# loss function and optimizers\ncriterion = nn.BCELoss()\n## different learning rates!!\noptimizer_G = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\nnum_generator_updates = 1\nlambda_gp = 10\nnum_epochs = 50 ## change this value\n## label smoothing!!\nreal_label = 1.0\nfake_label = 0.0\n\nimage_path = '/kaggle/input/div2k-high-resolution-images/DIV2K_train_HR/DIV2K_train_HR/0009.png'\n\n# training loop\nfor epoch in range(num_epochs):\n    generator.train()  # Set generator to training mode\n    discriminator.train()  # Set discriminator to training mode\n    \n    for i, (low_res, high_res) in enumerate(tqdm(train_loader)):\n        # move data to the correct device\n        low_res = low_res.to(device)\n        high_res = high_res.to(device)\n        \n        # add noise to the real and fake images\n        noise_std_dev = 0.00  # standard deviation for the noise; adjust as needed\n        noisy_real_images = high_res + noise_std_dev * torch.randn_like(high_res).to(device)\n        noisy_real_images = torch.clamp(noisy_real_images, 0.0, 1.0)  # Keep values within valid range [0, 1]\n\n        # generate fake images and add noise\n        fake_images = generator(low_res)\n        noisy_fake_images = fake_images + noise_std_dev * torch.randn_like(fake_images).to(device)\n        noisy_fake_images = torch.clamp(noisy_fake_images, 0.0, 1.0)  # Keep values within valid range [0, 1]\n        \n\n        # real and fake labels\n        output_real = discriminator(noisy_real_images).view(-1)\n        labels_real = torch.full((output_real.size(0),), real_label, dtype=torch.float, device=device)\n        loss_real = criterion(output_real, labels_real)\n\n        # train with noisy fake images\n        output_fake = discriminator(noisy_fake_images.detach()).view(-1)\n        labels_fake = torch.full((output_fake.size(0),), fake_label, dtype=torch.float, device=device)\n        loss_fake = criterion(output_fake, labels_fake)\n\n        # compute gradient penalty (if applicable)\n#         gradient_penalty = compute_gradient_penalty(discriminator, high_res, fake_images, device)\n        gradient_penalty = 1\n    \n        # total discriminator loss with gradient penalty\n        loss_D = loss_real + loss_fake + lambda_gp * gradient_penalty\n        loss_D.backward()\n\n        # update discriminator\n        optimizer_D.step()\n\n\n        # train generator\n        # multiple generator updates!!\n        for _ in range(num_generator_updates):\n            optimizer_G.zero_grad()\n\n            # generate fake images and calculate loss for the generator\n            fake_images = generator(low_res)  # Forward pass\n            output_fake_for_G = discriminator(fake_images).view(-1)\n            labels_fake_for_G = torch.full((output_fake_for_G.size(0),), real_label, dtype=torch.float, device=device)\n            loss_G = criterion(output_fake_for_G, labels_fake_for_G)\n            loss_G.backward()\n\n            # update generator\n            optimizer_G.step()\n        \n        # print progress\n        if i % 10 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i}/{len(train_loader)}], '\n                  f'Loss_D: {loss_real.item() + loss_fake.item():.4f}, Loss_G: {loss_G.item():.4f}')\n\n    # save models\n    torch.save(generator.state_dict(), f'generator_epoch_{epoch+1}.pth')\n    torch.save(discriminator.state_dict(), f'discriminator_epoch_{epoch+1}.pth')\n    \n    run_inference(generator, image_path)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}